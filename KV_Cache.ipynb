{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc2fd69-f666-4e1f-84c5-bd2b6bd8a09e",
   "metadata": {},
   "source": [
    "# KV Cache with GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3380bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3714c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeLU\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4aa8daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFN\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "                nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "                GELU(),\n",
    "                nn.Linear(cfg[\"emb_dim\"] * 4, cfg[\"emb_dim\"])\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5ae24c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    def __init__(self, max_batch_size, max_seq_len, n_kv_heads, head_dim):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.cache_k = torch.zeros((max_batch_size, max_seq_len, n_kv_heads, head_dim))# .to(device)\n",
    "        self.cache_v = torch.zeros((max_batch_size, max_seq_len, n_kv_heads, head_dim))# .to(device)\n",
    "\n",
    "    def update(self, batch_size, start_pos, xk, xv):\n",
    "        if self.cache_k.device != xk.device:\n",
    "            self.cache_k = self.cache_k.to(xk.device)\n",
    "            self.cache_v = self.cache_v.to(xv.device)\n",
    "        self.cache_k[:batch_size, start_pos :start_pos + xk.size(1)] = xk\n",
    "        self.cache_v[:batch_size, start_pos :start_pos + xv.size(1)] = xv\n",
    "\n",
    "    def get(self, batch_size, start_pos, seq_len):\n",
    "        keys = self.cache_k[:batch_size,  :start_pos + seq_len]\n",
    "        values = self.cache_v[:batch_size, :start_pos + seq_len]\n",
    "        return keys, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "693297bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHA\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False, use_kvcache=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        assert d_out % num_heads == 0, \"d_out is not divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "             torch.tril(torch.ones(context_length, context_length))\n",
    "        )\n",
    "        if use_kvcache:\n",
    "            print(\"Using KV Cache\")\n",
    "        self.kv_cache = KVCache(1, context_length, num_heads, d_out // num_heads) if use_kvcache else None\n",
    "        self.prefilled = False\n",
    "        self.context_length = context_length\n",
    "        self.num_tokens = 0\n",
    "\n",
    "    def forward_using_kvcache(self, x, ):\n",
    "\n",
    "        # print(x)\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        queries = self.W_query(x) # (b, 1, d_in) @ (d_in, d_out) -> (b, 1, d_out)\n",
    "        keys = self.W_key(x)      # (b, 1, d_in) @ (d_in, d_out) -> (b, 1, d_out)\n",
    "        values = self.W_value(x)  # (b, 1, d_in) @ (d_in, d_out) -> (b, 1, d_out)\n",
    "\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2) # (b, num_heads, c, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        \n",
    "        k_cache, v_cache = self.kv_cache.get(b, 0, self.num_tokens)\n",
    "        \n",
    "\n",
    "        \n",
    "        keys = torch.cat((k_cache[:, -self.context_length+1:], keys), dim=1)\n",
    "        keys = keys.transpose(1,2) # (b, num_heads, c, head_dim)\n",
    "        \n",
    "        values = torch.cat((v_cache[:, -self.context_length+1:], values), dim=1)\n",
    "        values = values.transpose(1,2) # (b, num_heads, c, head_dim)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(-1, -2) # (b, num_heads, 1, head_dim) @ (b, num_heads, head_dim, c) -> (b, num_heads, 1, c)\n",
    "        attn_scores = attn_scores / (keys.shape[-1]  ** 0.5)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1) # (b, num_heads, 1, c)\n",
    "\n",
    "        context_vec = attn_weights @ values # (b, num_heads,1, c) x (b, num_heads, c, head_dim) -> (b, num_heads, 1, head_dim)\n",
    "        context_vec = context_vec.transpose(1,2) # (b, 1, num_heads, head_dim)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        keys = keys.transpose(1,2) # (b, num_tokens, num_heads, head_dim)\n",
    "        values = values.transpose(1,2) # (b, num_tokens, num_heads, head_dim)\n",
    "        \n",
    "        self.kv_cache.update(1, 0, keys, values)\n",
    "        self.num_tokens = keys.shape[1]\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.prefilled and self.kv_cache:\n",
    "            return self.forward_using_kvcache(x)\n",
    "\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        queries = self.W_query(x) # (N, C, d_out)\n",
    "        keys = self.W_key(x) # (N, C, d_out)\n",
    "        values = self.W_value(x) # (N, C, d_out)\n",
    "\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2) # (b, num_heads, c, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1,2) # (b, num_heads, c, head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1,2) # (b, num_heads, c, head_dim)\n",
    "\n",
    "        \n",
    "        attn_scores = queries @ keys.transpose(-1, -2) # (b, num_heads, c, head_dim) @ (b, num_heads, head_dim, c) -> (b, num_heads, c, c)\n",
    "        attn_scores.masked_fill_(self.mask[:num_tokens, :num_tokens] == 0, float('-inf'))\n",
    "        attn_scores = attn_scores / (keys.shape[-1] ** 0.5)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1) # (b, num_heads, c, c)\n",
    "        \n",
    "        context_vec = attn_weights @ values # (b, num_heads,c, head_dim)\n",
    "        context_vec = context_vec.transpose(1,2) # (b, c, num_heads, head_dim)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        if self.kv_cache is not None:\n",
    "            self.kv_cache.update(1, 0, keys.transpose(1,2), values.transpose(1,2)) # prefill the cache\n",
    "            self.prefilled = True\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caec32b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(d_in=cfg[\"emb_dim\"],\n",
    "                                      d_out=cfg[\"emb_dim\"],\n",
    "                                      context_length=cfg[\"context_length\"],\n",
    "                                      dropout=cfg[\"drop_rate\"],\n",
    "                                     num_heads=cfg[\"n_heads\"],\n",
    "                                     qkv_bias=cfg[\"qkv_bias\"],\n",
    "                                     use_kvcache=cfg[\"use_kvcache\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = nn.LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = nn.LayerNorm(cfg['emb_dim'])\n",
    "        self.drop_resid = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x,)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + residual\n",
    "        \n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + residual\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd75dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT Class\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = nn.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "        #  self.tok_emb.weight = self.out_head.weight # Weight tying\n",
    "\n",
    "    def forward(self, in_idx, pos_ids):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(pos_ids)\n",
    "        \n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        \n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3234fac7",
   "metadata": {},
   "source": [
    "## Setting up tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48455a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f867de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size, use_kvcache=False):\n",
    "    if not use_kvcache:\n",
    "        for i in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -context_size:]\n",
    "            with torch.no_grad():\n",
    "                pos_id = torch.arange(idx_cond.shape[-1], device=idx_cond.device)\n",
    "                logits = model(idx_cond, pos_id)\n",
    "\n",
    "            logits = logits[:, -1, :]\n",
    "            probas = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "    else:\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            pos_id = torch.arange(idx_cond.shape[-1], device=idx_cond.device)\n",
    "            logits = model(idx_cond, pos_id)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "        for i in range(max_new_tokens-1):\n",
    "            idx_cond = idx[:, -context_size:]\n",
    "            with torch.no_grad():\n",
    "                pos_id = torch.arange(idx_cond.shape[-1], device=idx_cond.device)\n",
    "                logits = model(idx_cond[:,-1:], pos_id[-1:])\n",
    "\n",
    "            logits = logits[:, -1, :]\n",
    "            probas = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "        \n",
    "        for n,m in model.named_modules():\n",
    "            if m.__class__.__name__ == \"MultiHeadAttention\":\n",
    "                m.prefilled = False\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6188b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e856c311",
   "metadata": {},
   "source": [
    "## Model Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f44c6bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size\n",
    "    \"context_length\": 100,      # Context length\n",
    "    \"emb_dim\": 4,       # Embedding dimension\n",
    "    \"n_heads\": 2,        # Number of attention heads\n",
    "    \"n_layers\": 1,       # Number of layers\n",
    "    \"drop_rate\": 0.1,     # Dropout rate\n",
    "    \"qkv_bias\": False,     # Query-Key-Value bias\n",
    "    \"use_kvcache\": False,  # Enable Key-Value cache during inference\n",
    "}\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size\n",
    "    \"context_length\": 1024,      # Context length\n",
    "    \"emb_dim\": 768,       # Embedding dimension\n",
    "    \"n_heads\": 12,        # Number of attention heads\n",
    "    \"n_layers\": 12,       # Number of layers\n",
    "    \"drop_rate\": 0.1,     # Dropout rate\n",
    "    \"qkv_bias\": False,     # Query-Key-Value bias\n",
    "    \"use_kvcache\": False,  # Enable Key-Value cache during inference\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a860d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(GPTModel, config, device):\n",
    "    model = GPTModel(config).to(device)\n",
    "    print(f\"Using Device: {device}\")\n",
    "    print(f\"KV Cache Enabled: {config['use_kvcache']}\")\n",
    "    total_params = (sum([param.numel() for param in model.parameters()]))\n",
    "    total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "    print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9901f465-6394-471c-9901-3a1b3ec6190b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == \"cuda\":\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84107118",
   "metadata": {},
   "source": [
    "## Debug Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a627953f",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24691ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n",
      "KV Cache Enabled: False\n",
      "Number of trainable parameters considering weight tying: 201,668\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = setup_model(GPTModel, DEBUG_GPT_CONFIG_124M, device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4667f1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2061,   318,   509,    53, 40918,    30, 29406, 18396, 19475, 17925,\n",
      "         17925,  8979, 29406, 18396, 22769, 46648, 46648,  6998,  5698, 24090,\n",
      "         46648, 33004, 46648, 25028, 17925, 17925, 25028,  9891, 25891, 25891,\n",
      "          6998, 17050,   721,  5698, 17050, 25028, 46648, 25028, 17925, 46538,\n",
      "         25028, 19475, 17925, 17925,  1410, 17638, 46648, 33004, 18396,  6998,\n",
      "         37321, 17925, 17925, 46648, 16557, 22772, 17925, 41729, 25562, 18396,\n",
      "         25623, 17925, 19475, 17925,  4579, 46648, 33004, 25028, 19475,  2463,\n",
      "         17477,  1426,  5698, 17050, 17050,  8979, 17925, 17050,  5698, 25028,\n",
      "         45166, 33004, 17050, 17050, 12276, 46648,  7658, 46648,  5698, 18396,\n",
      "         16493, 17925, 38586, 25562,  7658, 12276, 10249, 18396, 17925, 17050,\n",
      "          8979, 17925, 42342, 17050,  8979]], device='cuda:0')\n",
      "What is KV caching? mish dude critically Harm HarmFile mish dudecharacterreetingsreetingsRS guidegemreetingsStrongreetings Herman Harm Harm Herman cheese abruptly abruptlyRS compilerec guide compiler Hermanreetings Herman Harm filament Herman critically Harm Harm plan medicationsreetingsStrong dudeRSConn Harm Harmreetingsudden Jill HarmZip persuaded dude confessed Harm critically HarmGBreetingsStrong Herman critically compan192 pos guide compiler compilerFile Harm compiler guide HermanorescStrong compiler compiler pirreetingsupidreetings guide dude outlined HarmTa persuadedupid pir 1991 dude Harm compilerFile Harmleigh compilerFile\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"What is KV caching?\"\n",
    "enc_text = text_to_token_ids(text, tokenizer)\n",
    "idx = enc_text.to(device)\n",
    "for _ in range(99):\n",
    "    idx_cond = idx[:, -DEBUG_GPT_CONFIG_124M[\"context_length\"]:]\n",
    "    with torch.no_grad():\n",
    "        pos_id = torch.arange(idx_cond.shape[-1], device=idx_cond.device)\n",
    "        logits = model(idx_cond, pos_id)\n",
    "\n",
    "        logits = logits[:, -1:, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_idx = torch.argmax(probs, dim=-1)\n",
    "\n",
    "        idx = torch.cat([idx, next_idx], dim=-1)\n",
    "# print(\"idx: \", idx)\n",
    "    \n",
    "baseline_idx = idx\n",
    "print(idx)\n",
    "print(token_ids_to_text(idx, tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740de286",
   "metadata": {},
   "source": [
    "## Using KV Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c4b6104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using KV Cache\n",
      "Using Device: cuda\n",
      "KV Cache Enabled: True\n",
      "Number of trainable parameters considering weight tying: 201,668\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "DEBUG_GPT_CONFIG_124M[\"use_kvcache\"] = True\n",
    "model = setup_model(GPTModel, DEBUG_GPT_CONFIG_124M, device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf281ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2061,   318,   509,    53, 40918,    30]], device='cuda:0')\n",
      "idx:  tensor([[ 2061,   318,   509,    53, 40918,    30, 29406, 18396, 19475, 17925,\n",
      "         17925,  8979, 29406, 18396, 22769, 46648, 46648,  6998,  5698, 24090,\n",
      "         46648, 33004, 46648, 25028, 17925, 17925, 25028,  9891, 25891, 25891,\n",
      "          6998, 17050,   721,  5698, 17050, 25028, 46648, 25028, 17925, 46538,\n",
      "         25028, 19475, 17925, 17925,  1410, 17638, 46648, 33004, 18396,  6998,\n",
      "         37321, 17925, 17925, 46648, 16557, 22772, 17925, 41729, 25562, 18396,\n",
      "         25623, 17925, 19475, 17925,  4579, 46648, 33004, 25028, 19475,  2463,\n",
      "         17477,  1426,  5698, 17050, 17050,  8979, 17925, 17050,  5698, 25028,\n",
      "         45166, 33004, 17050, 17050, 12276, 46648,  7658, 46648,  5698, 18396,\n",
      "         16493, 17925, 38586, 25562,  7658, 12276, 10249, 18396, 17925, 17050,\n",
      "          8979, 17925, 42342, 17050,  8979]], device='cuda:0')\n",
      "What is KV caching? mish dude critically Harm HarmFile mish dudecharacterreetingsreetingsRS guidegemreetingsStrongreetings Herman Harm Harm Herman cheese abruptly abruptlyRS compilerec guide compiler Hermanreetings Herman Harm filament Herman critically Harm Harm plan medicationsreetingsStrong dudeRSConn Harm Harmreetingsudden Jill HarmZip persuaded dude confessed Harm critically HarmGBreetingsStrong Herman critically compan192 pos guide compiler compilerFile Harm compiler guide HermanorescStrong compiler compiler pirreetingsupidreetings guide dude outlined HarmTa persuadedupid pir 1991 dude Harm compilerFile Harmleigh compilerFile\n"
     ]
    }
   ],
   "source": [
    "text = \"What is KV caching?\"\n",
    "enc_text = text_to_token_ids(text, tokenizer)\n",
    "idx = enc_text.to(device)\n",
    "idx_cond = idx[:, -DEBUG_GPT_CONFIG_124M[\"context_length\"]:]\n",
    "with torch.no_grad():\n",
    "    print(idx_cond)\n",
    "    pos_ids = torch.arange(idx_cond.shape[-1], device=idx_cond.device)\n",
    "    logits = model(idx_cond, pos_ids)\n",
    "    \n",
    "    logits = logits[:, -1:, :]\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    next_idx = torch.argmax(probs, dim=-1)\n",
    "    \n",
    "    idx = torch.cat([idx, next_idx], dim=-1)\n",
    "\n",
    "for _ in range(98):\n",
    "    idx_cond = idx[:, -DEBUG_GPT_CONFIG_124M[\"context_length\"]:]\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        pos_ids = torch.arange(idx_cond.shape[-1], device=idx_cond.device)\n",
    "        logits = model(idx_cond[:,-1:], pos_ids[-1:])\n",
    "\n",
    "        logits = logits[:, -1:, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_idx = torch.argmax(probs, dim=-1)\n",
    "\n",
    "        idx = torch.cat([idx, next_idx], dim=-1)\n",
    "\n",
    "using_kvcache_idx = idx\n",
    "print(\"idx: \", idx)\n",
    "print(token_ids_to_text(idx, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b465db66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KV Cache matches Baseline:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"KV Cache matches Baseline: \", torch.all(using_kvcache_idx == baseline_idx).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536428e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d2d483-db0f-4f34-8075-0c00ea9aeff1",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0af9de74-2eda-43d1-8942-668807a3ec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark():\n",
    "    torch.manual_seed(123)\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    print(f\"KV Cache Enabled: {GPT_CONFIG_124M['use_kvcache']}\")\n",
    "\n",
    "    total_params = (sum([param.numel() for param in model.parameters()]))\n",
    "    total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "    print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    text = \"What is KV caching?\"\n",
    "    encoded_text = text_to_token_ids(text, tokenizer)\n",
    "\n",
    "    model = model.eval()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Current Device: {device}\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    encoded_text = encoded_text.to(device)\n",
    "\n",
    "    times = []\n",
    "    for _ in tqdm(range(10)):\n",
    "        start = time.time()\n",
    "        _ = generate_text_simple(model, encoded_text, max_new_tokens=1000, context_size=GPT_CONFIG_124M['context_length'], use_kvcache=GPT_CONFIG_124M['use_kvcache'])\n",
    "        times.append((time.time() - start))\n",
    "    print(f\"{round(np.mean(times), 3)} +- {round(np.std(times), 3)} seconds\")\n",
    "\n",
    "\n",
    "    for n,m in model.named_modules():\n",
    "        if m.__class__.__name__ == \"MultiHeadAttention\":\n",
    "            m.prefilled = False\n",
    "\n",
    "    ids = generate_text_simple(model, encoded_text, max_new_tokens=30, context_size=GPT_CONFIG_124M['context_length'], use_kvcache=GPT_CONFIG_124M['use_kvcache'])\n",
    "    print(ids)\n",
    "    print(token_ids_to_text(ids, tokenizer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c1287b-ecb5-407e-a031-a28b62004dae",
   "metadata": {},
   "source": [
    "### Benchmarking Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b81027a2-f0d8-42b7-96c3-a610550384b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KV Cache Enabled: False\n",
      "Number of trainable parameters considering weight tying: 124,412,160\n",
      "Current Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:09<00:00, 18.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.967 +- 0.134 seconds\n",
      "tensor([[ 2061,   318,   509,    53, 40918,    30, 42435,  3686, 42043, 37759,\n",
      "         23338, 11760,  8219, 50042, 12132, 46106, 42954, 48984, 42924, 28790,\n",
      "         49285, 22055, 13968, 39926, 40144, 43959, 39492, 15209, 22104,   137,\n",
      "         20272, 47586,  5255, 22437, 17404, 47565]], device='cuda:0')\n",
      "What is KV caching? depletionurg Rooms Fabricن bandstenancewatching painful Maurit Rai Jah\".[ pleasingCHAativity embod bribes KiddKENpeed Nort Enhanced�arus counselling decredoes Creative peach\n"
     ]
    }
   ],
   "source": [
    "benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c907d7f-9610-439b-8c7c-7da9ddfd2a8a",
   "metadata": {},
   "source": [
    "### Benchmarking KV Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "324412f8-2978-47de-89a3-3595a63a7b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking with KV Cache\n",
      "Using KV Cache\n",
      "Using KV Cache\n",
      "Using KV Cache\n",
      "Using KV Cache\n",
      "Using KV Cache\n",
      "Using KV Cache\n",
      "Using KV Cache\n",
      "Using KV Cache\n",
      "Using KV Cache\n",
      "Using KV Cache\n",
      "Using KV Cache\n",
      "Using KV Cache\n",
      "KV Cache Enabled: True\n",
      "Number of trainable parameters considering weight tying: 124,412,160\n",
      "Current Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:38<00:00,  9.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.877 +- 0.371 seconds\n",
      "tensor([[ 2061,   318,   509,    53, 40918,    30, 42435,  3686, 42043, 37759,\n",
      "         23338, 11760,  8219, 50042, 12132, 46106, 42954, 48984, 42924, 28790,\n",
      "         49285, 22055, 13968, 39926, 40144, 43959, 39492, 15209, 22104,   137,\n",
      "         20272, 47586,  5255, 22437, 17404, 47565]], device='cuda:0')\n",
      "What is KV caching? depletionurg Rooms Fabricن bandstenancewatching painful Maurit Rai Jah\".[ pleasingCHAativity embod bribes KiddKENpeed Nort Enhanced�arus counselling decredoes Creative peach\n"
     ]
    }
   ],
   "source": [
    "print(\"Benchmarking with KV Cache\")\n",
    "GPT_CONFIG_124M['use_kvcache'] = True\n",
    "benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233cf9b5",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "1. Getting roughly 2x speed-up on GPT2 124M model.\n",
    "2. The new generated token needs to have its position embedding correctly set.\n",
    "3. Absolute Position embedding is not good for KV Cache. After reaching the max_seq_length, KV cache cannot be used\n",
    "because position embeddings in the KV Cache conflict with the new input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d7c2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
